---
title: "Does mere exposure to ideas encourage belief in them?"
bibliography: references.bib
csl: apa7.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Justin Mikell (jmikell@asu.edu)} \\ Arizona State University
    \AND {\large \bf Derek Powell (dmpowell@asu.edu)} \\ School of Social and Behavioral Sciences \\ Arizona State University}

abstract: >
    Numerous psychological findings have shown that mere exposure to ideas makes those ideas seem more true, a finding commonly referred to as the “illusory truth” effect [e.g. @hasher.etal1977]. In the presence of pervasive misinformation, this basic feature of cognition may undermine the functioning of a democratic society [@pennycook.etal2018]. However, genuine beliefs do not only produce judgments of truth, they also imply other beliefs and drive decision-making. Here, we sought to examine whether mere exposure to statements produces genuine beliefs by examining whether people draw inferences from statements after mere exposure. Surprisingly, and in contrast to familiarity-based accounts of the illusory truth effect [e.g. @dechene.etal2010], we found that exposure to “premise” statements affected participants’ truth ratings for novel “implied” statements. This “illusory implication” effect suggests that exposure to false statements has further-reaching impacts than previously thought and calls for a new mechanistic account of these effects.

    
keywords: >
    Illusory truth; Metacognition; Cognitive Psychology; Misinformation
    
output: 
  cogsci2016::cogsci_paper
    
final-submission: \cogscifinalcopy

# header-includes:
#   - \usepackage{float}
---


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r}
library(tidyverse)
library(patchwork)

df <- read_csv("../data/illusory-truth-main.csv") %>% 
  mutate(
    response = ordered(response, levels = c("Definitely False", "Probably False", "Maybe False", "Maybe True", "Probably True", "Definitely True"))
  ) %>% 
  mutate(resp_num = recode(response,
                           `Definitely True` = 2.5,
                           `Probably True` = 1.5,
                           `Maybe True` = .5,
                           `Maybe False` = -.5,
                           `Probably False` = -1.5,
                           `Definitely False` = -2.5)
         ) %>% 
  mutate(
    B_implied_true = ifelse(item_pair %in% c("war", "whitehouse", "bezos", "jackson",
                                          "indonesia", "eiffel", "newyork", "presidents",
                                          "tour", "africa", "disney", "avengers"), 
                            "F implied T", "T implied F"),
    B_implied_true = relevel(factor(B_implied_true), ref = "T implied F"),
      acc_num = case_when(
        item_type=="B" & B_implied_true == "F implied T" ~ -1*resp_num,
        item_type=="B" & B_implied_true != "F implied T" ~ resp_num,
        item_type=="A" ~ -1*resp_num,
        item_type=="Falsehood" ~ -1*resp_num,
        TRUE ~ resp_num
        ),
    acc_ordered = ordered(acc_num)
  )
```

```{r fit-models-exp1, include=F}
library(brms)

## Experiment 1

## Premises

fit_premise_fact <- brm(
  response ~ exposed + (1|subj_id) + (1|item_pair),
  data = df %>% 
    filter(phase=="test", item_type=="A", exposure_cond=="fact"),
  chains = 4,
  cores = 4,
  family=cumulative(),
  file="../local/fit_premise_fact"
)

fit_premise_quiz <- brm(
  response ~ exposed + (1|subj_id) + (1|item_pair),
  data = df %>% 
    filter(phase=="test", item_type=="A", exposure_cond=="quiz"),
  chains = 4,
  cores = 4,
  family=cumulative(),
  file="../local/fit_premise_quiz"
)

## within-subjs test

df_reg_within <- df %>% 
  filter(exposure_cond=="quiz") %>%
  filter(item_type == "A") %>% 
  ungroup() %>% 
  drop_na(response) %>% 
  select(subj_id, condition, exposed, exposure_cond, item_pair, phase, response)

## this is a bit needlessly complex but it is equivalent to testing the difference
fit_premise_within <- brm(
  response ~ phase + (1|subj_id:item_pair),
  data = df_reg_within,
  chains = 4,
  cores = 4,
  family=cumulative(),
  file="../local/fit_premise_within"
)

## Implications

fit_imp_fact <- brm(
  acc_ordered ~ exposed*B_implied_true + (1|subj_id) + (1|item_pair),
  data = df %>% 
    filter(phase=="test", item_type=="B", exposure_cond=="fact"),
  family = cumulative(),
  chains = 4,
  cores = 4,
  file="../local/fit_imp_fact"
)

# fit_imp_quiz <- brm(
#   acc_ordered ~ exposed + (1|subj_id) + (1|item_pair),
#   data = df %>% 
#     filter(phase=="test", item_type=="B", exposure_cond=="quiz"),
#   family = cumulative(),
#   chains = 4,
#   cores = 4,
#   file="../local/fit_imp_quiz"
# )

df_reg <- df %>% 
  mutate(
    familiarity = exposed,
    implication = case_when(
      B_implied_true == "F implied T" & exposed==1 ~ 1,
      B_implied_true == "T implied F" & exposed==1 ~ -1,
      TRUE ~ 0
    )
    # implied_true = if_else(B_implied_true == "F implied T" & exposure==1, 1, 0),
    # implied_false = if_else(B_implied_true != "F implied T" & exposure==1, 1, 0),
    ) %>% 
    filter(phase=="test", item_type=="B")


fit_imp_quiz_int <- brm(
  response ~ familiarity + implication + B_implied_true + 
    (1 + familiarity + implication|subj_id) + 
    (1 + familiarity + implication|item_pair),  # familiarity + implication + grouping (optional)
  data = df_reg %>% filter(exposure_cond=="quiz"),
  family = cumulative(),
  chains = 4,
  cores = 4,
  file="../local/fit_imp_quiz_int_max"
)

fit_imp_fact_int <- brm(
  response ~ familiarity + implication + B_implied_true + 
    (1 + familiarity + implication|subj_id) + 
    (1 + familiarity + implication|item_pair),  # familiarity + implication + grouping (optional)
  data = df_reg %>% filter(exposure_cond=="fact"),
  family = cumulative(),
  chains = 4,
  cores = 4,
  file="../local/fit_imp_fact_int_max"
)

```


```{r load-exp2, include=F}

df2 <- read_csv("../data/illusory-truth-main2.csv") %>% 
  mutate(
    response = ordered(response, levels = c("Definitely False", "Probably False", "Maybe False", "Maybe True", "Probably True", "Definitely True"))
  ) %>% 
  mutate(resp_num = recode(response,
                           `Definitely True` = 2.5,
                           `Probably True` = 1.5,
                           `Maybe True` = .5,
                           `Maybe False` = -.5,
                           `Probably False` = -1.5,
                           `Definitely False` = -2.5)
         ) %>% 
  mutate(
    B_implied_true = ifelse(item_pair %in% c("war", "whitehouse", "bezos", "jackson",
                                          "indonesia", "eiffel", "newyork", "presidents",
                                          "tour", "africa", "disney", "avengers"), 
                            "F implied T", "T implied F"),
    B_implied_true = relevel(factor(B_implied_true), ref = "T implied F"),
      acc_num = case_when(
        item_type=="B" & B_implied_true == "F implied T" ~ -1*resp_num,
        item_type=="B" & B_implied_true != "F implied T" ~ resp_num,
        item_type=="A" ~ -1*resp_num,
        item_type=="Falsehood" ~ -1*resp_num,
        TRUE ~ resp_num
        ),
    acc_ordered = ordered(acc_num)
  )
```

```{r fit-models-exp2, include=F}

## Experiment 2

df_reg2 <- df2 %>% 
  mutate(
    familiarity = exposed,
    implication = case_when(
      B_implied_true == "F implied T" & exposed==1 ~ 1,
      B_implied_true == "T implied F" & exposed==1 ~ -1,
      TRUE ~ 0
    )
    # implied_true = if_else(B_implied_true == "F implied T" & exposure==1, 1, 0),
    # implied_false = if_else(B_implied_true != "F implied T" & exposure==1, 1, 0),
    ) %>% 
    filter(phase=="test", item_type=="B", exposure_cond=="interest")

## Implications

fit_imp_exp2 <- brm(
  response ~ familiarity + implication + B_implied_true + 
    (1 + familiarity + implication|subj_id) + 
    (1 + familiarity + implication|item_pair),  # familiarity + implication + grouping (optional)
  data = df_reg2,
  family = cumulative(),
  prior = set_prior("lkj(5)", class = "cor"),
  chains = 4,
  cores = 4,
  iter = 3000,
  control = list(adapt_delta=.9),
  file="../local/fit_imp_exp2_max"
)

# Premises

fit_prem_exp2 <- brm(
  response ~ exposed + (1 + exposed|subj_id) + (1 + exposed|item_pair),
  data = df2 %>%
    filter(phase=="test", item_type=="A"),
  family = cumulative(),
  chains = 4,
  cores = 4,
  file="../local/fit_prem_exp2_max"
)
```

```{r make-tables, include=F}
make_table_df <- function(fit){
  fit_sum <- summary(fit)

  tbl_sum <- as_tibble(fit_sum$fixed, rownames="Term") %>%
    # bind_rows(
    #   as_tibble(fit_sum$random$item, rownames="Term") %>%
    #     mutate(Term = paste0("item_",Term))
    # ) %>%
    # bind_rows(
    #   as_tibble(fit_sum$random$subj_id, rownames="Term") %>%
    #     mutate(Term = paste0("subj_",Term))
    # ) %>%
    select(-Rhat, -Est.Error, -Bulk_ESS, -Tail_ESS)

  return(tbl_sum)
}

make_table <- function(table_df, caption=NULL){

  kableExtra::kbl(table_df, digits = 2, booktabs = T, caption=caption, escape = F, format="latex") %>%
    kableExtra::kable_classic(full_width=F)
}

mytable1 <- make_table_df(fit_imp_quiz_int) %>% 
  rename(`$CI_{2.5\\%}$` = `l-95% CI`,`$CI_{97.5\\%}$` = `u-95% CI`) %>%
  mutate(
    Term = case_when(
      Term == "B_implied_trueFimpliedT" ~ "implication type",
      # grepl("Intercept\\[", Term) ~ gsub("Intercept\\[(.*)\\]","$\\\\alpha_{\\1}$", Term),
      Term == "item_sd(Intercept)" ~ "$\\sigma_{item}$",
      Term == "subj_sd(Intercept)" ~ "$\\sigma_{subj}$",
      TRUE ~ Term
    )
  ) %>% 
  make_table("Population coefficients of Bayesian regression model for Implication effects in Experiment 1.")

mytable2 <- make_table_df(fit_imp_exp2) %>% 
  rename(`$CI_{2.5\\%}$` = `l-95% CI`,`$CI_{97.5\\%}$` = `u-95% CI`) %>%
  mutate(
    Term = case_when(
      Term == "B_implied_trueFimpliedT" ~ "implication type",
      # grepl("Intercept\\[", Term) ~ gsub("Intercept\\[(.*)\\]","$\\\\alpha_{\\1}$", Term),
      grepl("familiarity", Term) ~ "familiarity",
      # Term == "item_sd(Intercept)" ~ "$\\sigma_{item}$",
      # Term == "subj_sd(Intercept)" ~ "$\\sigma_{subj}$",
      TRUE ~ Term
    )
  ) %>% 
  make_table("Population coefficients of Bayesian regression model for Implication effects in Experiment 2.")

```

```{r helpers, include=F}
report_reg_coef <- function(model, coef_name){
  
  res_table <- summary(model)$fixed
  
  res <- res_table[coef_name,] %>% 
    mutate_if(is.numeric, ~round(.x, digits=3))
  
  res_text <- paste0(res[[1,1]], ", 95% CI [", res[1,3],", ",res[1,4], "]")
  return(res_text)
}

# report_reg_coef(fit_imp_exp2, "familiarityTRUE")
```

```{r demographics, include=F}
df_demo <- df %>% 
  group_by(subj_id) %>% 
  summarize(
    age = first(age),
    gender = first(gender)
  )
  
df2_demo <- df2 %>% 
  group_by(subj_id) %>% 
  summarize(
    age = first(age),
    gender = first(gender)
  )

exp1_n_f <- df_demo %>% 
  filter(gender=="Female") %>% 
  nrow()

exp2_n_f <- df2_demo %>% 
  filter(gender=="Female") %>% 
  nrow()

exp1_age <- median(df_demo$age, na.rm=TRUE)
exp2_age <- median(df2_demo$age, na.rm=TRUE)
```


Every day, people are faced with a barrage of unsupported claims, from pestering ad campaigns to blatant disinformation on social media. Altogether, we live within an information environment that is more connected and more saturated than ever before in human history [@bak-coleman.etal2021]. If we are sufficiently critical consumers of media, can we benefit from this rich access to information without being exploited by advertisers or misled by misinformation? Perhaps not. Numerous psychological findings indicate that mere exposure to ideas makes those ideas appear more true, a finding commonly referred to as the “illusory truth” effect [@dekeersmaecker.etal2020; @dechene.etal2010; @fazio.etal2015; @hasher.etal1977; @pennycook.etal2018]. This effect suggests that we cannot exist in an environment of mis- and disinformation without being affected by it---without exposure to these ideas distorting our sense of what is true. 

Studies examining the illusory truth effect typically proceed in at least two phases. At exposure, participants are introduced to a set of false statements. Typically, the statements are part of a true/false quiz, or a cover story explains they are part of some other innocuous judgment task. Then, after some intervening time ranging from minutes to weeks, participants are asked to judge whether these statements are true. On average, participants rate the statements as more “true” when they have been exposed to them previously---an illusory truth effect. 

Decades of research have demonstrated the consistency and robustness of the illusory truth effect [e.g. see @dechene.etal2010]. The effect has been demonstrated for frivolous trivia questions [e.g. @hasher.etal1977; @lacassagne.etal2021; @unkelbach2007; @wang.etal2016] as well as consequential fake news headlines [@pennycook.etal2018]. The illusory truth effect has also been shown to be robust across people with different levels of cognitive ability, need for cognitive closure, and cognitive styles [@dekeersmaecker.etal2020].

The effect is one of "illusory" truth because it occurs following "mere exposure" to statements. That is, it occurs when the statements are seen or heard in a non-communicative context, such as when they are read during a true/false quiz. Generally, being told something---even by a complete stranger of unknown trustworthiness---is prima facie reason for believing it [@grice1989]. But reading a statement on a true/false quiz is not reason for believing it---the statement is being shown only to test the quiz-taker's knowledge, and is just as likely to be false as to be true.

There is something unsettling about the illusory truth effect and the lack of agency it implies over our own beliefs. Even more worrisome, there could be dire societal implications if merely being exposed to an idea causes people to adopt it as a belief: Pennycook and colleagues [-@pennycook.etal2018] argue that the illusory truth effect, combined with an environment of pervasive misinformation, has important implications for the functioning of democratic society. The illusory truth effect suggests that mere exposure to misinformation could have impacts that cannot be stopped by fact-checking labels [@pennycook.etal2018], nor effectively curbed by retractions or corrective information [e.g. @ecker.etal2011; @lewandowsky.etal2012]. 

However, there is more to _belief_ than ratings of truth. To illustrate, philosophers advancing dispositionalist theories of beliefs have focused on a number of observable behaviors that are indicative of belief [@schwitzgebel2021]. Though a person’s assent to a proposition (whether they agree with or judge it to be truthful) is an important marker and convenient measure of belief, other features of belief are just as essential. For instance, beliefs imply other beliefs: for instance, believing that “it is sunny out” implies the belief that “it is daytime”. And believing misinformation, such as “Hillary Clinton runs a sex-trafficking ring out of a pizza parlor” [see @robb2017] might imply the belief that “Hillary Clinton would make a bad president.” Beliefs also inform action and decision-making, and whether or not someone holds a belief can be judged (at least partly) from their decisions. When truly believed, misinformation can have drastic impacts. To illustrate, a recent study found that people who marked just one piece of vaccine misinformation as accurate were nearly twice as likely to be unvaccinated against COVID-19 compared to those who did not endorse any vaccine misinformation [@ognyanova.etal2021]. 

Bearing these features of beliefs in mind, the current state of illusory truth research does not demonstrate that mere exposure to a statement truly engenders _belief_ in that statement. Consider the mechanisms thought to underlie the illusory truth effect: most explanations attribute illusory truth effects to participants’ reliance on familiarity or fluency during the judgment process [@unkelbach2007; @wang.etal2016; @fazio.etal2015]. Roughly, when asked whether something is true or false, people search their memory for knowledge pertaining to belief in the statement, but they also rely on a meta-cognitive sense of familiarity. Familiarity is a sign that we have “heard this somewhere before.” According to foundational theories of communication, a general assumption that communicators strive to make true utterances is a prerequisite for successful communication and social functioning [@grice1989]. Thus, if having “heard something before” suggests that someone said it, then ecologically this is a reasonable cue to truth. Psychological studies that elicit the illusory truth effect hijack this meta-cognitive heuristic: They provide this sense of familiarity, but from a communicative context that lacks any reasonable assumption of truth.

This predominant theoretical account suggests that illusory truth effects do not engender genuine belief and may not have the deleterious real-world effects that some authors have feared [c.f. @pennycook.etal2018]. Applying this mechanistic account to the real-world, we might summarize the most likely process as follows: When people are exposed to a false statement they form a memory impression of it, but they do not draw inferences from this statement to revise other beliefs, nor do they plan future actions on its basis. In a real-world context, there would be essentially no impact from the exposure. In the experimental context however, the statement is seen again and a truth-judgment is requested. It is only at this time that the memory impression of the statement produces a feeling of familiarity and affects the truth judgments rendered at that time. Under this theoretical account, the impact of prior exposure depends on a subsequent re-exposure, coupled with a judgment or action to be taken at that time. This would make the real-world consequences of the illusory truth effect far narrower than some have argued.

Nevertheless, much as we might be relieved to dismiss them, concerns like those raised by Pennycook and colleagues [-@pennycook.etal2018] loom large enough to warrant further consideration. Could mere exposure to statements truly impact beliefs beyond providing a sense of familiarity? Could illusory truth effects instead reflect genuine belief? 

Here, we sought to examine whether the illusory truth effect reflects genuine beliefs in statements following their mere exposure. One way to test this is to examine whether people draw inferences from statements after exposure to those statements in a context without any presupposition of truth. We designed a study to examine whether mere exposure to one statement (the “premise”) could affect truth ratings for another, different statement that it would logically imply (the “implication”). Such an “illusory implication” effect would demonstrate that mere-exposure has genuine impacts on beliefs beyond those explained by familiarity. In contrast, if the illusory truth effect is caused entirely by feelings of familiarity during the judgment process, then there should be no effect of prior exposure for new unfamiliar statements.


# Experiment 1

Preregistration information for Experiment 1 can be found at https://osf.io/c4w8s/. Materials for both experiments can be found at https://osf.io/znq3y/.

## Methods


### Participants

A total of 400 Participants were recruited through Amazon Mechanical Turk. Participants who failed basic attention check questions were excluded from analyses. These questions asked participants to simply give a particular response. For example, "This item tests if you are paying attention, please select 'Definitely False'." At the end of the study, participants were also asked if they had searched online to check any of the claims during the experiment (after being reassured that they would receive compensation regardless of their answers). Participants who failed any attention checks or who indicated they searched online were excluded from analysis. This left a final sample of `r nrow(df_demo)` (`r exp1_n_f` female, median age `r exp1_age` years-old).

```{r fig1, fig.env="figure", fig.pos = "h", fig.align = "center", fig.height=2.5, fig.width=3, fig.cap="Average truth ratings for Implication statements in Experiment 1, broken down by exposure (exposed vs. unexposed), instructions (fact vs quiz conditions), implication type (true-implied-false vs false-implied-true). For visualization purposes, means were calcaulated by translating ordinal responses onto a scale from -2.5 to 2.5. Error bars indicate standard errors. "}
fig1 <- df %>% 
  filter(phase=="test") %>% 
  filter(item_type == "B") %>% 
  group_by(exposed, exposure_cond, B_implied_true) %>% 
  summarize(
    M = mean(resp_num),
    se = sd(resp_num)/sqrt(n()),
    ll = M - se,
    ul = M + se
  ) %>% 
  mutate(
    exposure_cond = if_else(exposure_cond=="fact", "Fact condition", "Quiz condition"),
    exposed = if_else(exposed==1, "Exposed", "Unexposed"),
    B_implied_true = as.character(B_implied_true)
    ) %>% 
  ggplot(
    aes(x=B_implied_true, color=factor(exposed), shape=factor(exposed), y = M, ymin = ll, ymax=ul)
  ) +
  geom_pointrange(position=position_dodge(width=.5)) +
  # coord_flip() +
  scale_color_manual(values = c("#da1e28", "#0f62fe")) +
  labs(y = "Truth rating", x = "Implication type", color="Exposure", shape="Exposure") +
  facet_wrap(~exposure_cond) +
  theme_bw(base_size=9.5) +
  theme(panel.grid=element_blank(), legend.position="bottom") +
  theme(axis.text.x = element_text(angle = 10, vjust = 0.5, hjust=.5))

fig1
```

### Materials

We created 24 pairs of statements presenting claims about history. Each pair consisted of a "premise" statement and a "implied" statement. Each “premise” statement was a falsehood that implied either the truth or falsity of the “implied” statement. Of the pairs, 12 had premise statements implying that a true statement was false (true-implied-false) and the other 12 had premise statements implying another false statement was true (false-implied-true). For instance, one "true-implied-false" pair was the premise statement, “No U.S. astronauts have died since the Challenger explosion in 1986” paired with the implied statement, “The space shuttle Columbia disintegrated over Texas in 2003” (true, but implied to be false by the "premise"). An example of a "false-implied-true" pair was the premise statement, "The Tour de France has been held every year since its inception in 1903" paired with the implied statement, "The Tour de France was still held during WWI and WWII" (false, but implied true by the "premise").

```{r, include=F}
# library(kableExtra)
# kableExtra::kable(
#   tibble(
#     Type = c("True implied false", "False implied true"),
#     Premise = c(
#       "The Tour de France has been held every year since its inception in 1903 (False)",
#       "Henry Ford Invented the first automobile with the Model T (False)"
#     ),
#     Implied = c(
#       "The Tour de France was still held during WWI and WWII (False, implied True)",
#       "Karl Benz patented and sold the first automobile in 1886 (True, implied false)"
#     )
#   ),
#   caption = "Examples of premise and implied statement pairs"
# ) %>% 
#   column_spec(1, width = "4em") %>% 
#   column_spec(2, width = "10em") %>% 
#   column_spec(3, width = "10em")
  
```

### Procedures

Participants first completed informed consent and a Captcha to limit bot participation. 

The study proceeded in three phases: 1) an exposure phase, 2) a distraction phase, 3) and finally a testing phase. At the exposure phase, participants were presented with a subset of the "premise" statements (8 of the 24 total) as well as some control statements (12 true, 4 false). Then, after the distraction phase, the testing phase consisted of a true-or-false quiz. The items tested included both "premise" and "implied" statements from the 24 item-pairs.  For some of the tested "implied" statements, participants had previously seen the related "premise" statement (exposed implied test). For others, they had not seen the related statements (unexposed implied test). Similarly, they were tested on the "premise" statements they had previously seen during the exposure phase (exposed premise test) as well as new unseen premise statements (for which they had also not seen the related implied statements). Participants were randomly assigned into three counterbalancing conditions that varied which of the statement-pairs were assigned to each exposure/test combination.

At the beginning of the study, participants were randomly assigned to either the “fact” (_n_ = 100) or “quiz” (_n_ = 300) exposure condition and to one of three counterbalancing conditions. Participants assigned to the quiz and fact conditions received different instructions and performed different tasks in the exposure phase. 

Participants in the fact condition were told the study’s main purpose was to learn more about how people learn and apply new knowledge. They were informed that the initial set of statements they would see were all facts, and were asked to rate how surprising each “fact” was to them. The true purpose of this condition was to test whether participants would successfully draw inferences from the "premise" to the "implied" statements when told that the premises were true.

Participants in the quiz condition were told that they were to be given a true/false quiz, so that some of the statements would be true and some false. These participants rated how confident they were that each of the presented statements were true or false. The purpose of the quiz condition was to provide "mere exposure" to these statements, to test for illusory truth and illusory implication effects. We tested for the illusory truth effect by comparing truth ratings for premise statements seen during exposure to those that had not been seen. Similarly, we tested for the illusory implication effect by comparing truth ratings for implication statements whose corresponding premise statements were and were not presented during the exposure phase. If mere exposure to "premise" statements affects participants' judgments of the "implied" statements, this would be evidence for an illusory implication effect.

After their initial exposure to the statements, participants provided basic demographic information and were then presented with the expanded 7 question version of the Cognitive Reflection Task [CRT, @frederick2005; @toplak.etal2014]. These tasks served to provide a period of distraction between exposure and test.

Then, participants advanced to the testing phase, where all participants were asked to rate their confidence that each statement was true or false. These ratings were made on a 6-point scale from "Definitely False" to "Definitely True". The test phase consisted of the 8 premise statements the participants previously saw, their respective matching 8 implication statements, 8 novel implication statements, 8 previously unseen premise statements, 12 true control statements, and 4 false control statements.

At the end of the study, participants were debriefed and presented with a list of the false statements they had seen. 


## Results and discussion

Participants made their truth ratings in the test phase on a 6-point scale from “Definitely false” to “Definitely true”. To properly treat these Likert-style responses, the truth ratings were analyzed using multilevel Bayesian cumulative ordinal regression models [@burkner.vuorre2019] with random intercepts and slopes for participants and items. Cumulative ordinal regression models assume that participants’ discrete responses are driven by a continuous latent variable and a set of k-1 thresholds determining the range of the continuous variable corresponding to each of the ordinal response options. This model helps to account for potential differences in scale usage as well as the bounded nature of the response scale.

All models were fit using the brms R package [@burkner2017], with model posteriors estimated using the No-U-Turn Markov Chain Monte Carlo (MCMC) sampler implemented in Stan. Four MCMC chains were run for each model, with 2000 samples (1000 burn-in) drawn from each. Chains were assessed for convergence with $\hat{R}$ and the total estimated effective sample size was verified to be greater than 1000 for all parameters [@gelman.etal2014].



### Fact condition

First, we examined truth ratings for the premise and implications statements among participants who were told that the false statements were true at exposure (“fact” condition). As expected, exposure to the premise statements when presented as “facts” increased endorsement at test, $\beta =$ `r report_reg_coef(fit_premise_fact, "exposed")`. In addition, participants successfully drew inferences from these “facts”, resulting in decreased accuracy for the implication statements at test, $\beta =$ `r report_reg_coef(fit_imp_fact, "exposed")`.


### Quiz condition

It is quite appropriate that people should draw new inferences when they learn new facts. But would participants’ endorsements of the implication statements be affected by mere exposure to the premise statements in the “quiz” condition? To our surprise, it appears that they were. Figure 1 shows participants’ average truth ratings for the implication statements in the quiz condition. Participants gave higher truth ratings following exposure for false statements implied to be true, and somewhat lower truth ratings following exposure for true statements implied to be false. 


```{r, showtable1, echo=F, message=F}
mytable1
```


```{r, showtable2, echo=F, message=F}
mytable2
```


An increase in truth ratings for the false-implied-true statements could potentially be explained by familiarity. This would be consistent with findings from Arkes and colleagues [-@arkes.etal1991], who found evidence for illusory truth for novel statements that were on the same topic as previously-seen statements. However, familiarity cannot explain the decrease in truth ratings for the true-implied-false items, as familiarity should encourage uniformly higher, not lower, truth ratings.

A regression model was used to tease apart the effects of familiarity and logical implication. The model includes a) a binary variable indicating the type of implication statement (true-implied-false or false-implied-true), b) a binary predictor for prior exposure to the related premise coded zero when the premise was not seen and 1 when it was seen (capturing potential effects of familiarity), c) and a variable capturing the effect of implications, coded 1 for implied truth, -1 for implied falsehood, and zero when the related premise statement was not seen at exposure. Thus the "implication" predictor accounts for the effect of exposure to the premise on truth judgments for the implied statements (either positive or negative), while the "familiarity" predictor accounts for any positive effect of familiarity. We also incorporated "maximal" random intercepts and slopes for all terms varying by subject and item [@barr.etal2013]. Expressed in the common “lme4” syntax [@bates.etal2015], the regression model:

\begin{align*}
\text{response} \sim 
 \text{item type} &+ \text{familiarity} + \text{implication} \\
&+ (1 + \text{familiarity} + \text{implication}|\text{subject}) \\
&+ (1 + \text{familiarity} + \text{implication}|\text{item})
\end{align*}

Table 1 presents a summary of the posterior distribution estimated for the population-level coefficients in this model. The results indicate that both familiarity and the implication of the premise statements affected participants’ truth ratings. The implication effect is very credibly greater than zero. To judge its overall magnitude, we compared the parameter estimate to the parameter estimate from the same regression model applied to participants’ responses in the “fact” condition ($\beta =$ `r report_reg_coef(fit_imp_fact_int, "implication")`). Although smaller than the effect observed in the “fact” condition, the effect in the “quiz” condition is of a similar order of magnitude.

Surprisingly, the effects of exposure on participants’ truth ratings for the premise statements were very subtle. Our primary preregistered test compared participants’ truth ratings for the exposed premise statements at test against their ratings for another set of 8 unexposed premise statements. This analysis did not find any evidence of an illusory truth effect, with a posterior estimate for the exposure parameter that credibly included zero. However, a secondary preregistered analysis comparing participants’ initial truth judgments for the premise-statements to their later truth judgments for those same statements at test did find a credible increase in endorsements, $\beta =$ `r report_reg_coef(fit_premise_within, "phasetest")`.

The small size of these illusory truth effects may owe to the use of a “quiz” judgment task at exposure as well as the relatively brief period of distraction between exposure and test used in Experiment 1. By first responding to the statements in a quiz, participants may then have felt pressure to maintain consistency in their responses, and may have still possessed some explicit memory of their responses for the premise statements. If so, this could have suppressed the influence of familiarity traditionally argued to produce the illusory truth effect. 

More importantly, a drive to maintain consistency might also explain our surprising findings of an illusory implication effect. Participants showed a general bias toward judging statements “true” during the exposure phase, responding with some variation of “true” for 71.8% of responses. Attempting to maintain consistency with these prior responses could have thus influenced their responses to the implication statements, rendering the observed effect an artifact of the experimental context.


# Experiment 2

Experiment 2 was conducted to address the possibility that a pressure for internal consistency produced the effects on the implication-statement truth ratings observed in Experiment 1. Preregistration informationExperiment 2 can be found at https://osf.io/czkdh/.


```{r fig2, fig.env="figure", fig.pos = "t", fig.align = "center", fig.height=2.5, fig.width=2.5, fig.cap="Average truth ratings for Implication statements in Experiment 2, broken down by exposure (exposed vs. unexposed) and implication type (true-implied-false vs false-implied-true). For visualization purposes, means were calcaulated by translating ordinal responses onto a scale from -2.5 to 2.5. Error bars indicate standard errors."}

fig2 <- df2 %>% 
  filter(phase=="test") %>% 
  mutate(resp_num = recode(response,
                           `Definitely True` = 2.5,
                           `Probably True` = 1.5,
                           `Maybe True` = .5,
                           `Maybe False` = -.5,
                           `Probably False` = -1.5,
                           `Definitely False` = -2.5)
  ) %>% 
  mutate(
    B_implied_true = ifelse(item_pair %in% c("war", "whitehouse", "bezos", "jackson",
                                          "indonesia", "eiffel", "newyork", "presidents",
                                          "tour", "africa", "disney", "avengers"), "F implied T", "T implied F"),
      acc_num = ifelse(B_implied_true == "F implied T", -1*resp_num, resp_num)
  ) %>% 
  filter(item_type == "B") %>% 
  mutate(
    exposed = if_else(exposed, "Exposed", "Unexposed")
  ) %>% 
  group_by(exposed, exposure_cond, B_implied_true) %>% 
  summarize(
    M = mean(resp_num),
    se = sd(resp_num)/sqrt(n()),
    ll = M - se,
    ul = M + se
  ) %>% 
  ggplot(
    aes(x=B_implied_true, color=factor(exposed), shape=factor(exposed), y = M, ymin = ll, ymax=ul)
  ) +
  geom_pointrange(position=position_dodge(width=.5)) +
  scale_color_manual(values = c("#da1e28", "#0f62fe")) +
  theme_bw(base_size=10) +
  theme(legend.position="bottom", panel.grid=element_blank()) +
  # coord_flip() +
  labs(y = "Truth rating", x = "Implication type", color="Exposure", shape="Exposure")

fig2
```


## Methods


### Participants 

A total of 300 participants were recruited from Amazon Mechanical Turk using procedures identical to Experiment 1. As before, participants who failed attention check questions or who indicated they had looked up answers were excluded from analyses, leaving a final sample of `r nrow(df2_demo)` (`r exp2_n_f` female, median age `r exp2_age` years-old).


### Materials and procedures

All items and procedures were identical to Experiment 1 except for three changes. 

First, all participants were assigned to a new “interest” condition. At exposure, participants were told that they would see a set of statements, some true and some false, and they were instructed to rate how interesting each statement was. This change was made to avoid forcing participants to make a true/false judgment for the exposed premise-statements, which could have pushed them to try to make coherent or consistent responses to the related implication-statements. Importantly, as with a true/false quiz, the presentation of statements in this context should not warrant any inference as to the truth of those statements. 

Secondly, to provide a longer period of distraction between exposure and test, the ordering of the items in the test phase was rearranged. Participants first rated 24 control items (12 true and 12 false) to extend the time between rating main test items. 

Third, another change in test presentation order further prevented any consistency-pressure: Participants judged the truth of all of the implication statements before judging the premise statements.

## Results and discussion

Figure 2 shows participants’ average truth ratings for the implication statements with and without exposure to their corresponding premise statements in Experiment 2. The illusory implication effect was again observed. As shown in the figure, truth ratings were clearly affected by exposure: participants gave higher truth ratings following exposure for false statements implied to be true, and lower truth ratings following exposure for true statements implied to be false. Table 2 shows the posterior population-level estimates for an identical regression as was conducted for Experiment 1. Experiment 2 replicated the effects of Experiment 1 in a revised design that eliminated any consistency pressure or demands on participants. In fact, the magnitude of the effect for the implication statements was somewhat larger in Experiment 2 than in Experiment 1. 


```{r fig3, fig.env="figure*", fig.pos = "!hbt", fig.align = "center", fig.width=6.5, fig.height=3, fig.cap="Average truth ratings for individual premise and implication statements in Experiment 2, broken down by exposure (exposed vs. unexposed) and implication type (true-implied-false vs false-implied-true). For visualization purposes, means were calcaulated by translating ordinal responses onto a scale from -2.5 to 2.5. Error bars indicate standard errors.", num.cols.cap=2}

df2_plotting <- df2 %>% 
  filter(phase=="test") %>% 
  mutate(resp_num = recode(response,
                           `Definitely True` = 2.5,
                           `Probably True` = 1.5,
                           `Maybe True` = .5,
                           `Maybe False` = -.5,
                           `Probably False` = -1.5,
                           `Definitely False` = -2.5)
  ) %>% 
  mutate(
    B_implied_true = ifelse(item_pair %in% c("war", "whitehouse", "bezos", "jackson",
                                          "indonesia", "eiffel", "newyork", "presidents",
                                          "tour", "africa", "disney", "avengers"), "F implied T", "T implied F")
  ) %>% 
  mutate(exposed = if_else(exposed, "Exposed", "Unexposed"))

df2_extra <- df2_plotting %>% 
  filter(phase=="test") %>% 
  filter(item_type == "A") %>% 
  group_by(exposed, exposure_cond, item_pair, B_implied_true) %>% 
  summarize(
    M = mean(resp_num),
    # se = sd(resp_num)/sqrt(n()),
    # ll = M - se,
    # ul = M + se
  ) %>% 
  spread(exposed, M) %>% 
  mutate(diff = abs(Exposed-Unexposed)) %>% 
  select(-Exposed, -Unexposed)

plt_A <- df2_plotting %>% 
  filter(phase=="test") %>% 
  filter(item_type == "A") %>% 
  group_by(exposed, exposure_cond, item_pair, B_implied_true) %>% 
  summarize(
    M = mean(resp_num),
    se = sd(resp_num)/sqrt(n()),
    ll = M - se,
    ul = M + se
  ) %>% 
  left_join(df2_extra) %>% 
  ggplot(
    aes(x=reorder(item_pair, diff), color=factor(exposed), shape=factor(exposed), y = M, ymin = ll, ymax=ul)
  ) +
  geom_pointrange(position=position_dodge(width=.5)) +
  scale_color_manual(values = c("#da1e28", "#0f62fe")) +
  coord_flip() +
  labs(title = "Premise-statements", y = "Truth rating", x= "Item", color="Exposure", shape="Exposure") +
  theme_bw(base_size=9) +
  theme(legend.position="right", legend.direction="vertical",panel.grid=element_blank()) +
  facet_wrap(~B_implied_true, scales="free", ncol=1)

# plt_A

plt_B <- df2_plotting %>%  
  filter(phase=="test") %>% 
  filter(item_type == "B") %>% 
  group_by(exposed, exposure_cond, item_pair, B_implied_true) %>% 
  summarize(
    M = mean(resp_num),
    se = sd(resp_num)/sqrt(n()),
    ll = M - se,
    ul = M + se
  ) %>% 
  left_join(df2_extra) %>% 
  ggplot(
    aes(x=reorder(item_pair, diff), color=factor(exposed), shape=factor(exposed),  y = M, ymin = ll, ymax=ul)
  ) +
  geom_pointrange(position=position_dodge(width=.5)) +
  scale_color_manual(values = c("#da1e28", "#0f62fe")) +
  coord_flip() +
  labs(title = "Implication-statements", y = "Truth rating", x= "Item", color="Exposure", shape="Exposure") +
  theme_bw(base_size=9) +
  theme(legend.position="right", legend.direction="vertical", panel.grid=element_blank()) +
  facet_wrap(~B_implied_true, scales="free", ncol=1)


fig3 <- (plt_A + plt_B + guide_area()) + plot_layout(widths=c(4,4,1.5), heights=5, guides="collect")

fig3 # fix legend
```

In addition, participant’s truth ratings for the premise statements in Experiment 2 revealed the classic illusory truth effect: premise statements were rated as more true when participants had been exposed to them previously $\beta =$ `r report_reg_coef(fit_prem_exp2, "exposedTRUE")`. 

Interestingly, though somewhat smaller, the illusory implication effect for the implication statements was generally similar in magnitude to the illusory truth effect observed among the premise statements.

One limitation of these studies relative to prior work on the illusory truth effect is the relatively limited number of statements participants saw and were tested on. Generating diverse pairs of statements with a clear implication relation is relatively more challenging than simply generating false statements. Fortunately as shown in Figure 3, the illusory implication effect was fairly robust across the majority of the 24 different individual item pairs in Experiment 2.

# Discussion 

We observed an “illusory implication” effect across two preregistered experiments: mere exposure to premises (e.g. that “The Tour de France has been held every year since its inception in 1903”) influenced participant’s truth ratings for examples of their logical implications (e.g. that “The Tour de France was still held during World War I and World War II”). Unlike the illusory truth effect, this novel effect cannot be easily explained by the effects of familiarity or fluency on the judgment process [cf.  @fazio.etal2015; @unkelbach2007; @wang.etal2016]: whereas familiarity would be expected to uniformly increase endorsements, participants’ truth ratings for implication statements were both increased and decreased according to the implications of the corresponding premises. Thus, it appears that mere exposure has a genuine impression on people’s beliefs beyond simply creating a sense of familiarity.

If not familiarity, then what cognitive mechanism could explain these findings? One potential explanation is that mere exposure to the premise statements essentially leads participants to at least partially adopt the premise statements as beliefs. Simply entertaining the idea may create such an impression despite awareness of its ambiguous truth-value. Or, consistent with a general overriding expectation of truth in testimony [e.g. @grice1989; @levine2014], participants may at least partially accept the premise statements as true despite the experimental context making clear they should not. This explanation would be consistent with the general truth-bias observed throughout both studies. Future research might vary the plausibility of the premise statements to evaluate this explanation or identify its boundary conditions [@fazio.etal2019].

Another potential explanation would attribute the illusory implication effect to processes that play out in the testing phase. Rather than familiarity, the illusory implication effect may instead reflect explicit memory and source misattribution: Participants may (at least partially) remember the premise statements, but fail to properly attribute their source to the experimental context. Future research might explore how explicit memory for the premise statements correlates with the strength of the illusory implication effects. In particular, research might disentangle memory effects by exploring factors that are known to affect memory, but that should not affect other reasoning processes, such as retention intervals or serial-position effects [e.g. @bjork.whitten1974].

Finally, given the surprising nature of our findings, we must raise the possibility of potential deflationary explanations: that the finding could be some kind of experimental demand characteristic. For instance, participants’ might imagine there is some kind of trick so that they are expected to reason forward from the initial exposure to the items. Or, some proportion might have been sufficiently confused so as to imagine they were meant to believe the statements on their initial exposure (though it is hard to imagine this in Experiment 1). Further work should explore participants' perceptions of the purposes of these studies and the potential for different response strategies. However, it should be noted that similar concerns could be levied at essentially all prior research on the illusory truth effect.

Whatever the cognitive mechanism behind the illusory implication effect we observed, these findings are further cause for concern about the spread of misinformation on and offline. The spread of misinformation and fake news may outpace the spread of factual news [@vosoughi.etal2018] and creates massive challenges for online platforms seeking to remove, label, and correct misinformation [@sharma.etal2019]. These challenges may be further compounded by basic features of human cognition: mere exposure to false statements can make them appear true. Further, as our findings are the first to indicate, these impacts generalize to related statements and are potentially indicative of genuine belief following mere exposure.


# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
